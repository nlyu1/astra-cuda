import random
import time
import gc
import os
os.environ["TORCHINDUCTOR_CACHE_DIR"] = os.path.expanduser("~/.cache/torch_compile")
os.environ["TORCH_COMPILE_CACHE_DIR"] = os.path.expanduser("~/.cache/torch_compile")

import numpy as np
import torch
torch.backends.cudnn.benchmark = True
torch.set_float32_matmul_precision('medium')
torch._dynamo.config.cache_size_limit = 256
import wandb
from pathlib import Path
from tqdm import tqdm
import sys 
import argparse

sys.path.append('./src')

from high_low.agent import HighLowTransformerModel
from high_low.config import Args
from high_low.env import HighLowTrading
from high_low.logger import HighLowLogger
from high_low.vtrace import HighLowVTraceTrainer, HighLowVTraceBuffer
from timer import Timer, SegmentTimer

# Parse command line argument for project directory
parser = argparse.ArgumentParser()
parser.add_argument("project_dir", type=str, help="Path to project directory")
cmd_args = parser.parse_args()

python_root = Path(__file__).parent
project_dir = Path(cmd_args.project_dir)
assert project_dir.exists(), f"Project directory {project_dir} does not exist"

# Find latest main checkpoint
def sort_key_fn(x):
    name = x.stem 
    if name.startswith('main_'):
        return int(name.split('_')[-1])
    else:
        return -1e10

available_checkpoints = sorted(project_dir.glob('main_*.pt'), key=sort_key_fn, reverse=True)
if len(available_checkpoints) == 0:
    raise ValueError(f"Need to find checkpoint of name `main_<step>.pt` in {project_dir}")

# Load the latest main checkpoint
latest_main_path = available_checkpoints[0]
latest_main_name = latest_main_path.stem
print(f"Found latest main checkpoint: {latest_main_name}")

checkpoint = torch.load(latest_main_path, map_location='cpu', weights_only=False)
main_weights = checkpoint['model_state_dict']
args = checkpoint['args']

# Modify args for exploiter training
args.run_name = f"HighLowExploiter__{args.exp_name}__{args.seed}__{int(time.time())}"
args.self_play_prob = 0.0  # Never self-play, always play against the fixed main
args.fill_runtime_args()
print(args)

game_config = args.get_game_config()
env = HighLowTrading(game_config)
device = torch.device(f'cuda:{args.device_id}')

random.seed(args.seed)
np.random.seed(args.seed)
torch.manual_seed(args.seed)

# Initialize buffer and agents
num_features = env.num_features()
buffer = HighLowVTraceBuffer(args, num_features, device)

exploiter_agent = HighLowTransformerModel(args, env).to(device)
trainer = HighLowVTraceTrainer(args, exploiter_agent)

# Initialize NPC agents with the fixed main checkpoint weights
npc_agents = [
    HighLowTransformerModel(args, env, verbose=False).to(device)
    for _ in range(game_config['players'] - 1)]

for agent in npc_agents:
    agent.load_state_dict(main_weights, strict=True)

observation_buffer = env.new_observation_buffer()
reward_buffer = env.new_reward_buffer()
returns_buffer = env.new_reward_buffer()
player_reward_buffer = env.new_player_reward_buffer()

done_zeros, done_ones = torch.zeros(args.num_envs, device=device).float(), torch.ones(args.num_envs, device=device).float()

dist_params_buffer = {
    'center': torch.zeros(args.num_steps, 4, device=device),
    'precision': torch.zeros(args.num_steps, 4, device=device)}

# Training monitoring (persistent across rounds)
timer = Timer()
segment_timer = SegmentTimer()
logger = HighLowLogger(args)
gc.disable()

# Training loop that continuously finds the latest main and trains exploiters against it.
while True:
    # Reset exploiter to random weights for each round
    exploiter_agent = HighLowTransformerModel(args, env).to(device)
    trainer = HighLowVTraceTrainer(args, exploiter_agent)
    
    # Reset tracking variables
    ewm_alpha = 0.01  # For 0.99-EWM (more conservative)
    ewm_avg_returns = 0.0
    ewm_update_count = 0
    global_step = 0
    
    print(f"\nStarting new exploiter training round against {latest_main_name}")
    pbar = tqdm(range(args.num_iterations))
    segment_timer.tick('init_agents')
    
    for iteration in pbar:
        if iteration > 0 and iteration % 200 == 0:
            gc.collect()
    
        player_offset = np.random.randint(0, game_config['players']) # Random exploiter offset 
        
        # Reset contexts
        for agent in npc_agents:
            agent.reset_context()
        exploiter_agent.reset_context()

        ### Rollout ###
        segment_timer.tick('rollout')
        settlement_preds, private_role_preds = [], []
        buffer.pinfo_tensor = env.pinfo_tensor()
        
        for step in range(args.num_steps):
            global_step += args.num_envs 

            for npc_id in range(player_offset):
                assert (env.current_player() == npc_id)
                env.fill_observation_tensor(observation_buffer)
                with torch.autocast(device_type=device.type, dtype=torch.bfloat16, cache_enabled=True):
                    with torch.inference_mode():
                        torch.compiler.cudagraph_mark_step_begin()
                        npc_actions = npc_agents[npc_id].incremental_forward(observation_buffer, step)['action']
                env.step(npc_actions)
            
            # Exploiter action
            assert env.current_player() == player_offset
            if step > 0:
                env.fill_rewards_since_last_action(buffer.rewards[step - 1])
                buffer.update_late_stats({'dones': done_zeros}, step - 1)

            env.fill_observation_tensor(buffer.obs[step])
            with torch.autocast(device_type=device.type, dtype=torch.bfloat16, cache_enabled=True):
                with torch.inference_mode():
                    torch.compiler.cudagraph_mark_step_begin()
                    forward_results = exploiter_agent.incremental_forward(buffer.obs[step], step)
            
            action, log_probs = forward_results['action'], forward_results['logprobs']
            settlement_preds.append(forward_results['pinfo_preds']['settle_price'].clone())
            private_role_preds.append(forward_results['pinfo_preds']['private_roles'].argmax(dim=-1))

            for k, v in forward_results['action_params'].items():
                dist_params_buffer[k][step] = v.mean(0)
            
            buffer.update({
                'actions': action,
                'logprobs': log_probs,
            }, step)
            
            env.step(action)

            # Remaining player actions
            for player_id in range(player_offset + 1, game_config['players']):
                npc_id = player_id - 1
                assert (env.current_player() == player_id)
                env.fill_observation_tensor(observation_buffer)
                with torch.autocast(device_type=device.type, dtype=torch.bfloat16, cache_enabled=True):
                    with torch.inference_mode():
                        npc_actions = npc_agents[npc_id].incremental_forward(observation_buffer, step)['action']
                env.step(npc_actions)

        assert env.terminal()
        segment_timer.tick('update_buffer')
        env.fill_returns(returns_buffer)
        env.fill_rewards_since_last_action(buffer.rewards[step], player_offset)
        buffer.update_late_stats({
            'rewards': player_reward_buffer,
            'dones': done_ones}, step)

        # Get environment info
        env_info = env.expose_info()
        env_pinfo_targets = env.get_pinfo_targets()
        
        # Update EWM average returns
        exploiter_returns = returns_buffer[:, player_offset].mean().item()
        if ewm_update_count == 0:
            ewm_avg_returns = exploiter_returns
        else:
            ewm_avg_returns = (1 - ewm_alpha) * ewm_avg_returns + ewm_alpha * exploiter_returns
        ewm_update_count += 1

        # Logging
        segment_timer.tick('logging')
        settlement_preds_stacked = torch.stack(settlement_preds, dim=0)
        logging_inputs = {
            'returns': returns_buffer,
            'offset': player_offset,
            'settlement_preds': settlement_preds_stacked,
            'private_role_preds': torch.stack(private_role_preds, dim=0),
            'infos': env_info | env_pinfo_targets,
            'dist_params': dist_params_buffer,
            'segment_timer': {f'performance/{k}': v for k, v in segment_timer.elapsed_times.items()}}
        
        logger.update_stats(logging_inputs, global_step, heavy_updates=False)
        
        # Log exploiter-specific metrics
        wandb.log({
            "exploiter/avg_returns": exploiter_returns,
            "exploiter/ewm_avg_returns": ewm_avg_returns,
            "exploiter/steps": global_step,
            "exploiter/trained_against": latest_main_name,
        }, step=global_step)

        # Populate buffer's actual private info
        buffer.actual_private_roles.copy_(env_pinfo_targets['pinfo_targets'], non_blocking=True)
        buffer.actual_settlement.copy_(env_pinfo_targets['settlement_values'], non_blocking=True)
        
        # Reset environment
        env.reset()

        # Training step
        segment_timer.tick('trainer step')
        running_sps = timer.tick(args.num_envs * args.num_steps)
        wandb.log({"performance/SPS": running_sps}, step=global_step)

        update_dictionary = buffer.get_update_dictionary()
        trainer_results = trainer.train(update_dictionary)
        wandb.log(trainer_results, step=global_step)

        # Check completion criteria
        if global_step > 3000 and ewm_avg_returns > 0.1:
            training_complete = True
            completion_step = global_step
            print(f"Training complete at step {global_step} with EWM avg returns: {ewm_avg_returns:.4f}")
            
            # Save the exploiter checkpoint to push to main pool
            save_path = project_dir / f"exploiter_{latest_main_name}_{global_step}.pt"
            torch.save({'model_state_dict': exploiter_agent.state_dict()}, save_path)
            print(f"Saved exploiter checkpoint to {save_path}")
            
            # Measure exploitability gap
            print("Measuring exploitability gap...")
            gap_returns = []
            for _ in range(10):  # 10 evaluation episodes
                env.reset()
                eval_returns = torch.zeros(args.num_envs, device=device)
                
                for step in range(args.num_steps):
                    for player_id in range(game_config['players']):
                        if player_id == 0:  # Exploiter always plays as player 0 for evaluation
                            env.fill_observation_tensor(observation_buffer)
                            with torch.inference_mode():
                                action = exploiter_agent.incremental_forward(observation_buffer, step)['action']
                        else:
                            env.fill_observation_tensor(observation_buffer)
                            with torch.inference_mode():
                                action = npc_agents[player_id - 1].incremental_forward(observation_buffer, step)['action']
                        env.step(action)
                
                env.fill_returns(returns_buffer)
                gap_returns.append(returns_buffer[:, 0].mean().item())
            
                exploitability_gap = np.mean(gap_returns)
                wandb.log({
                    "exploiter/exploitability_gap": exploitability_gap,
                    "exploiter/saved_at_step": global_step,
                }, step=global_step)
                
                print(f"Exploitability gap: {exploitability_gap:.4f}")
                print("Restarting with new random exploiter...\n")
                break  # Exit inner loop to start new round